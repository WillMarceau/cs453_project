{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mapping: {'glioma_tumor': 0, 'meningioma_tumor': 1, 'no_tumor': 2, 'pituitary_tumor': 3}\n",
      "Batch image shape: torch.Size([32, 3, 224, 224])\n",
      "Batch labels: tensor([3, 3, 1, 3, 1, 1, 3, 2, 3, 0, 2, 1, 1, 3, 0, 1, 0, 0, 3, 3, 0, 2, 2, 1,\n",
      "        0, 0, 1, 3, 0, 2, 0, 3])\n",
      "Training class distribution: Counter({3: 827, 0: 826, 1: 822, 2: 395})\n",
      "Testing class distribution: Counter({1: 58, 0: 50, 3: 37, 2: 27})\n",
      "Validation class distribution: Counter({1: 57, 0: 50, 3: 37, 2: 26})\n"
     ]
    }
   ],
   "source": [
    " # Define transformations (resize and convert to tensor)\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
    "    ])\n",
    "\n",
    "    # Define dataset directories\n",
    "train_dir = \"../data/Training\"\n",
    "test_dir = \"../data/Testing\"\n",
    "valid_dir = \"../data/Validation\"\n",
    "\n",
    "    # Load datasets\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "valid_dataset = datasets.ImageFolder(root=valid_dir, transform=transform)\n",
    "\n",
    "    # Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Print class-to-index mapping\n",
    "print(\"Class mapping:\", train_dataset.class_to_idx)\n",
    "\n",
    "    # Check a batch of images and labels\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"Batch image shape: {images.shape}\")\n",
    "print(f\"Batch labels: {labels}\")\n",
    "\n",
    "    # Checking the label counts\n",
    "label_counts = Counter(train_dataset.targets)\n",
    "print(\"Training class distribution:\", label_counts)\n",
    "\n",
    "label_counts = Counter(test_dataset.targets)\n",
    "print(\"Testing class distribution:\", label_counts)  \n",
    "\n",
    "label_counts = Counter(valid_dataset.targets)\n",
    "print(\"Validation class distribution:\", label_counts)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        # this function defines the nn structure\n",
    "        super().__init__()\n",
    "        self.cnn_model = nn.Sequential(\n",
    "\n",
    "            # conv layer 1\n",
    "            # 1 input as each pixel is level of gray\n",
    "            nn.Conv2d(in_channels = 3, out_channels=8, kernel_size=3),\n",
    "\n",
    "            # normalize batch to reduce overfitting\n",
    "            nn.BatchNorm2d(8),\n",
    "\n",
    "            #Relu activation, more commonly used than tanh\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            # pool elements\n",
    "            # might want to do kernel_size of 3\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 3),\n",
    "\n",
    "            # conv layer 2\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size = 3),\n",
    "\n",
    "             # normalize batch to reduce overfitting\n",
    "            nn.BatchNorm2d(16),\n",
    "\n",
    "            #Relu activation, more commonly used than tanh\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            # pool elements\n",
    "            # might want to do kernel_size of 3\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 4),\n",
    "        )\n",
    "\n",
    "        self.dense_model = nn.Sequential(\n",
    "            # adjust in_features based on performance\n",
    "            # first linear layer\n",
    "            nn.Linear(in_features=5184, out_features = 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # add droput layer to help with overfitting\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            # second\n",
    "            #nn.Linear(in_features=128, out_features = 64),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features = 4),\n",
    "            # softmax is already applied for cross entropy loss \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # this function defines how the information goes through the nn\n",
    "\n",
    "        # send through cnn\n",
    "        x = self.cnn_model(x)\n",
    "\n",
    "        # flatten output\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # send through dense \n",
    "        x = self.dense_model(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Get number of classes\n",
    "\n",
    "# Initialize the model\n",
    "model = CNN()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "class_counts = torch.tensor([826, 822, 395, 827])  # Your class distribution\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = class_weights / class_weights.sum()  # Normalize\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.5074, Val Loss: 1.5703, Val Acc: 35.88%\n",
      "Epoch 2, Train Loss: 0.6277, Val Loss: 1.7190, Val Acc: 38.24%\n",
      "Epoch 3, Train Loss: 0.5145, Val Loss: 1.6692, Val Acc: 41.18%\n",
      "Epoch 4, Train Loss: 0.4484, Val Loss: 1.8906, Val Acc: 41.76%\n",
      "Epoch 5, Train Loss: 0.3564, Val Loss: 1.6698, Val Acc: 49.41%\n",
      "Epoch 6, Train Loss: 0.3557, Val Loss: 1.5208, Val Acc: 58.24%\n",
      "Epoch 7, Train Loss: 0.3385, Val Loss: 1.5927, Val Acc: 52.94%\n",
      "Epoch 8, Train Loss: 0.2545, Val Loss: 1.9415, Val Acc: 55.88%\n",
      "Epoch 9, Train Loss: 0.1972, Val Loss: 1.8772, Val Acc: 52.94%\n",
      "Epoch 10, Train Loss: 0.1813, Val Loss: 1.9760, Val Acc: 50.59%\n",
      "Epoch 11, Train Loss: 0.1957, Val Loss: 2.2870, Val Acc: 55.29%\n",
      "Epoch 12, Train Loss: 0.1833, Val Loss: 2.1482, Val Acc: 62.94%\n",
      "Epoch 13, Train Loss: 0.1693, Val Loss: 2.3508, Val Acc: 58.82%\n",
      "Epoch 14, Train Loss: 0.1612, Val Loss: 2.0125, Val Acc: 68.82%\n",
      "Epoch 15, Train Loss: 0.1248, Val Loss: 2.1295, Val Acc: 67.65%\n",
      "Epoch 16, Train Loss: 0.1103, Val Loss: 1.9072, Val Acc: 72.35%\n",
      "Epoch 17, Train Loss: 0.1112, Val Loss: 2.4708, Val Acc: 70.00%\n",
      "Epoch 18, Train Loss: 0.1317, Val Loss: 2.3602, Val Acc: 55.88%\n",
      "Epoch 19, Train Loss: 0.1215, Val Loss: 2.6067, Val Acc: 56.47%\n",
      "Epoch 20, Train Loss: 0.1141, Val Loss: 2.3961, Val Acc: 69.41%\n",
      "Epoch 21, Train Loss: 0.0893, Val Loss: 2.4522, Val Acc: 70.00%\n",
      "Epoch 22, Train Loss: 0.0838, Val Loss: 2.0770, Val Acc: 71.76%\n",
      "Epoch 23, Train Loss: 0.0817, Val Loss: 2.8983, Val Acc: 63.53%\n",
      "Epoch 24, Train Loss: 0.0922, Val Loss: 2.1014, Val Acc: 71.76%\n",
      "Epoch 25, Train Loss: 0.0816, Val Loss: 2.4991, Val Acc: 69.41%\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
    "    model.to(device)  # Ensure model is on the correct device\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Accuracy calculation\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = correct / total * 100\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Train for 25 epochs\n",
    "train_model(model, train_loader, valid_loader, criterion, optimizer, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 67.44%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)\n",
    "evaluate_model(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall per class: {'Class 0': 0.20000000298023224, 'Class 1': 0.6896551847457886, 'Class 2': 1.0, 'Class 3': 0.9459459185600281}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_recall_without_sklearn(model, data_loader):\n",
    "    model.eval()\n",
    "    true_positives = torch.zeros(4)\n",
    "    false_negatives = torch.zeros(4)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            for i in range(4):\n",
    "                true_positives[i] += ((predicted == i) & (labels == i)).sum().item()\n",
    "                false_negatives[i] += ((predicted != i) & (labels == i)).sum().item()\n",
    "    \n",
    "    # Compute recall per class\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    recall[torch.isnan(recall)] = 0  # Handle division by zero cases\n",
    "\n",
    "    # Convert to dictionary\n",
    "    recall_dict = {f\"Class {i}\": recall[i].item() for i in range(4)}\n",
    "\n",
    "    return recall_dict\n",
    "\n",
    "# Example usage:\n",
    "recall_rates = calculate_recall_without_sklearn(model, test_loader)\n",
    "print(\"Recall per class:\", recall_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "          gliom  menin  no_tu  pitui\n",
      "     gliom    10      6     26      8\n",
      "     menin     3     40      2     13\n",
      "     no_tu     0      0     27      0\n",
      "     pitui     0      2      0     35\n",
      "\n",
      "Confusion Matrix:\n",
      "          gliom  menin  no_tu  pitui\n",
      "     gliom   826      0      0      0\n",
      "     menin    29    763      3     27\n",
      "     no_tu     0      0    394      1\n",
      "     pitui     0      0      0    827\n"
     ]
    }
   ],
   "source": [
    "def compute_confusion_matrix(model, data_loader):\n",
    "    model.eval()\n",
    "    confusion_matrix = torch.zeros(4, 4, dtype=torch.int64)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
    "                confusion_matrix[t, p] += 1  # Increment the matrix count\n",
    "\n",
    "    return confusion_matrix\n",
    "\n",
    "# Example usage:\n",
    "conf_matrix = compute_confusion_matrix(model, test_loader)\n",
    "test_conf = compute_confusion_matrix(model, train_loader)\n",
    "\n",
    "def print_confusion_matrix(conf_matrix, class_names):\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(\" \" * 10, end=\"\")  # Formatting for column headers\n",
    "    print(\"  \".join(f\"{cls[:5]:>5}\" for cls in class_names))  # Print class names as column headers\n",
    "\n",
    "    for i, row in enumerate(conf_matrix):\n",
    "        print(f\"{class_names[i][:5]:>10} \", end=\"\")  # Row labels\n",
    "        print(\"  \".join(f\"{val:5}\" for val in row.tolist()))  # Print matrix row values\n",
    "\n",
    "# Example usage:\n",
    "print_confusion_matrix(conf_matrix, train_dataset.classes)\n",
    "print_confusion_matrix(test_conf, train_dataset.classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
